{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF module\n",
    "### => 문서 내부의 특정 단어가 어느정도 빈도로 있는지 파악하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "from konlpy.tag import Okt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "word_dic = {\"_id\" : 0} # 단어가 들어가는 사전\n",
    "dt_dic = {} # 문장 전체에서의 단어 출현 횟수\n",
    "files = [] # 문서들을 저장할 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    result = []\n",
    "    \n",
    "    word_s = okt.pos(text, norm = True, stem = True)\n",
    "    \n",
    "    for n, h in word_s:\n",
    "        \n",
    "        if not (h in [\"Noun\", \"Verb\", \"Adjective\"]):\n",
    "            continue\n",
    "        if h == \"Punctuation\" and h2 == \"Number\":\n",
    "            continue\n",
    "            \n",
    "        result.append(n)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_dic(words, auto_add=True):\n",
    "    \n",
    "    # 단어를 id로 변환\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        if w in word_dic:\n",
    "            result.append(word_dic[w])\n",
    "        \n",
    "        elif auto_add:\n",
    "            id = word_dic[w] = word_dic[\"_id\"]\n",
    "            \n",
    "            word_dic[\"_id\"] += 1\n",
    "            \n",
    "            result.append(id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(text):\n",
    "    \n",
    "    # 텍스트를 id 리스트로 변환해서 추가\n",
    "    \n",
    "    ids = words_to_ids(tokenize(text))\n",
    "    files.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_file(path):\n",
    "    # 텍스트 파일을 학습용으로 추가\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        s = f.read()\n",
    "        add_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_files():\n",
    "    \n",
    "    # 추가한 파일 계산\n",
    "    \n",
    "    global dt_dic\n",
    "    result = []\n",
    "    doc_count = len(files)\n",
    "    dt_dic = {}\n",
    "    \n",
    "    # 단어 출현 횟수 세기\n",
    "    \n",
    "    for words in files:\n",
    "        \n",
    "        used_word = {}\n",
    "        \n",
    "        data = np.zeros(word_dic[\"_id\"])\n",
    "        \n",
    "        for id in words:\n",
    "            data[id] += 1\n",
    "            used_word[id] = 1\n",
    "        \n",
    "        # 단어 t가 사용되고 있을 경우 dt_dic의 수를 1 더함\n",
    "        \n",
    "        for id in used_word:\n",
    "            if not (id in dt_dic):\n",
    "                dt_dic[id] = 0\n",
    "            else:\n",
    "                dt_dic[id] += 1\n",
    "        \n",
    "        # 정규화\n",
    "        \n",
    "        data = data / len(words)\n",
    "        result.append(data)\n",
    "        \n",
    "    # tf-idf 계산\n",
    "    \n",
    "    for i, doc in enumerate(result):\n",
    "        \n",
    "        for id, v in enumerate(doc):\n",
    "            \n",
    "            idf = np.log(doc_count / dt_dic[id]) + 1\n",
    "            doc[id] = min([doc[id] * idf, 1.0])\n",
    "        result[i] = doc\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dic(fname):\n",
    "    \n",
    "    # dict를 파일로 저장\n",
    "    \n",
    "    pickle.dump([word_dic, dt_dic, files], open(fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dic(fname):\n",
    "    \n",
    "    # dict 파일 읽기\n",
    "    \n",
    "    global word_dic, dt_dic, files\n",
    "    \n",
    "    n = pickle.load(open(fname, \"rb\"))\n",
    "    word_dic, dt_dic, files = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_text(text):\n",
    "    \n",
    "    # 문장을 벡터로 변환\n",
    "    \n",
    "    data = np.zeros(word_dic[\"_id\"])\n",
    "    words = words_to_ids(tokenize(text), False)\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        data[w] += 1\n",
    "        \n",
    "    data = data / len(words)\n",
    "    \n",
    "    for id, v in enumerate(data):\n",
    "        \n",
    "        idf = np.log(len(files) / dt_dic[id]) + 1\n",
    "        data[id] = min([data[id] * idf, 1.0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531]), array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531]), array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531]), array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531]), array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531]), array([1., 0., 0., 0., 0., 0.]), array([0.34751987, 0.59338619, 0.44828016, 0.        , 0.        ,\n",
      "       0.        ]), array([0.20851192, 0.35603171, 0.2689681 , 0.51372318, 0.51372318,\n",
      "       0.        ]), array([0.34751987, 0.        , 0.44828016, 0.        , 0.        ,\n",
      "       0.85620531])]\n",
      "{'_id': 6, '비': 0, '오늘': 1, '내리다': 2, '덥다': 3, '오후': 4, '일요일': 5}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    add_text(\"비\")    \n",
    "    add_text(\"오늘은 비가 내렸어요.\")\n",
    "    add_text(\"오늘은 더웠지만 오후부터 비가 내렸다.\")\n",
    "    add_text(\"비가 내리는 일요일이다.\")\n",
    "    print(calc_files())\n",
    "    print(word_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분류하기\n",
    "\n",
    "1. 텍스트에서 불필요한 품사를 제거한다.\n",
    "2. dict를 기반으로 단어를 숫자로 변환(int 형 id로 변환)\n",
    "3. 파일 내부의 단어 출현 비율을 계산\n",
    "4. 여러 text로 데이터를 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 내부의 파일 목록 전체에 대한 처리\n",
    "\n",
    "def read_files(path, label):\n",
    "    \n",
    "    print(\"read_files=\", path)\n",
    "    \n",
    "    files = glob.glob(path + \"/*.txt\")\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        add_files(f)\n",
    "        \n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_files= text/100\n",
      "read_files= text/101\n",
      "read_files= text/103\n",
      "read_files= text/105\n"
     ]
    }
   ],
   "source": [
    "# 기사를 넣은 디렉토리 읽어들이기\n",
    "\n",
    "read_files(\"text/100\", 0)\n",
    "read_files(\"text/101\", 1)\n",
    "read_files(\"text/103\", 2)\n",
    "read_files(\"text/105\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 벡터로 변환\n",
    "\n",
    "x = calc_files()\n",
    "\n",
    "# save\n",
    "\n",
    "pickle.dump([y, x], open(\"text/genre.pickle\", \"wb\"))\n",
    "save_dic(\"text/genre-tfidf.dic\")\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
